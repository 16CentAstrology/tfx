{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFgojohE42wu"
   },
   "outputs": [],
   "source": [
    "##### Copyright \\u0026copy; 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ftGcLFZ42wx"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqZC7Cv942w0"
   },
   "source": [
    "# Create a TFX pipeline using templates with Beam orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRBoc5la42w0"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This document will provide instructions to create a TensorFlow Extended (TFX) pipeline\n",
    "using *templates* which are provided with TFX Python package.\n",
    "Most of instructions are Linux shell commands, and corresponding\n",
    "Jupyter Notebook code cells which invoke those commands using `!` are provided.\n",
    "\n",
    "You will build a pipeline using [Taxi Trips dataset](\n",
    "https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)\n",
    "released by the City of Chicago. We strongly encourage you to try to build\n",
    "your own pipeline using your dataset by utilizing this pipeline as a baseline.\n",
    "\n",
    "We will build a pipeline using [Apache Beam Orchestrator](https://www.tensorflow.org/tfx/guide/beam_orchestrator). If you are interested in using Kubeflow orchestrator on Google Cloud, please see [TFX on Cloud AI Platform Pipelines tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Linux / MacOS\n",
    "* Python >= 3.5.3\n",
    "\n",
    "You can get all prerequisites easily by [running this notebook on Google Colab](https://colab.sandbox.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/template_beam.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRW5apUw42w1"
   },
   "source": [
    "## Step 1. Set up your environment.\n",
    "\n",
    "**Throughout this document, we will present commands twice. Once as a copy-and-paste-ready shell command, once as a jupyter notebook cell. If you are using Colab, just skip shell script block and execute notebook cells.**\n",
    "\n",
    "You should prepare a development environment to build a pipeline.\n",
    "\n",
    "Install `tfx` python package. We recommend use of `virtualenv` in the local environment. You can use following shell script snippet to set up your environment.\n",
    "\n",
    "```sh\n",
    "# Create a virtualenv for tfx.\n",
    "virtualenv -p python3 venv\n",
    "source venv/bin/activate\n",
    "# Install python packages.\n",
    "python -m pip install --user --upgrade tfx==0.22.0\n",
    "```\n",
    "If you are using colab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llKzIjr442w1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: tensorflow-transform 0.21.2 has requirement tensorflow<2.2,>=1.15, but you'll have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: kubernetes 10.1.0 has requirement pyyaml~=3.12, but you'll have pyyaml 5.3.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: apache-beam 2.17.0 has requirement httplib2<=0.12.0,>=0.8, but you'll have httplib2 0.17.4 which is incompatible.\u001b[0m\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/Users/sujipark/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script fastavro is installed in '/Users/sujipark/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gen_client is installed in '/Users/sujipark/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tfx is installed in '/Users/sujipark/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user --upgrade -q tfx==0.21.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NA86S2v42w1"
   },
   "source": [
    "NOTE: There might be some errors during package installation. For example,\n",
    "\n",
    ">ERROR: some-package 0.some_version.1 has requirement other-package!=2.0.,&lt;3,&gt;=1.15, but you'll have other-package 2.0.0 which is incompatible.\n",
    "\n",
    "Please ignore these errors at this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6-DrWm042w4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/local/bin:/opt/local/sbin:/Library/Frameworks/Python.framework/Versions/3.7/bin:/Library/Frameworks/Python.framework/Versions/3.7/bin:/usr/local/bin:/usr/local/sbin:/Users/sujipark/opt/anaconda3/envs/tfx/bin:/Users/sujipark/opt/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/Users/sujipark/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include user python binary directory.\n",
    "HOME=%env HOME\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:{HOME}/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YszwBVuS42w6"
   },
   "source": [
    "Let's check the version of TFX.\n",
    "```bash\n",
    "python -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBLyQWYF42w6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "ModuleNotFoundError: No module named 'tfx'\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycspntQk42xF"
   },
   "source": [
    "And, it's done. We are ready to create a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BoSOcmEB42xF"
   },
   "source": [
    "## Step 2. Copy predefined template to your project directory.\n",
    "\n",
    "In this step, we will create a working pipeline project directory and files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a different name by changing the `PIPELINE_NAME` below. This will also become the name of the project directory where your files will be put.\n",
    "\n",
    "```bash\n",
    "export PIPELINE_NAME=\"my_pipeline\"\n",
    "export PROJECT_DIR=~/tfx/${PIPELINE_NAME}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYGyT4ib42xG"
   },
   "outputs": [],
   "source": [
    "PIPELINE_NAME=\"my_pipeline\"\n",
    "import os\n",
    "# Create a project directory under Colab content directory.\n",
    "PROJECT_DIR=\"/Users/sujipark/tfx/\"+PIPELINE_NAME #os.path.join(os.sep,\"content\",PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sujipark/tfx/my_pipeline'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GjXe_3oY42xI"
   },
   "source": [
    "TFX includes the `taxi` template with the TFX python package. If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "The `tfx template copy` CLI command copies predefined template files into your project directory.\n",
    "\n",
    "```sh\n",
    "tfx template copy \\\n",
    "   --pipeline_name=\"${PIPELINE_NAME}\" \\\n",
    "   --destination_path=\"${PROJECT_DIR}\" \\\n",
    "   --model=taxi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PmXatBD42xI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Copying taxi pipeline template\n",
      "beam_dag_runner.py -> /Users/sujipark/tfx/my_pipeline/beam_dag_runner.py\n",
      "configs.py -> /Users/sujipark/tfx/my_pipeline/pipeline/configs.py\n",
      "__init__.py -> /Users/sujipark/tfx/my_pipeline/pipeline/__init__.py\n",
      "pipeline.py -> /Users/sujipark/tfx/my_pipeline/pipeline/pipeline.py\n",
      "__init__.py -> /Users/sujipark/tfx/my_pipeline/__init__.py\n",
      "model_test.py -> /Users/sujipark/tfx/my_pipeline/models/keras/model_test.py\n",
      "constants.py -> /Users/sujipark/tfx/my_pipeline/models/keras/constants.py\n",
      "__init__.py -> /Users/sujipark/tfx/my_pipeline/models/keras/__init__.py\n",
      "model.py -> /Users/sujipark/tfx/my_pipeline/models/keras/model.py\n",
      "preprocessing_test.py -> /Users/sujipark/tfx/my_pipeline/models/preprocessing_test.py\n",
      "model_test.py -> /Users/sujipark/tfx/my_pipeline/models/estimator/model_test.py\n",
      "constants.py -> /Users/sujipark/tfx/my_pipeline/models/estimator/constants.py\n",
      "__init__.py -> /Users/sujipark/tfx/my_pipeline/models/estimator/__init__.py\n",
      "model.py -> /Users/sujipark/tfx/my_pipeline/models/estimator/model.py\n",
      "__init__.py -> /Users/sujipark/tfx/my_pipeline/models/__init__.py\n",
      "features.py -> /Users/sujipark/tfx/my_pipeline/models/features.py\n",
      "features_test.py -> /Users/sujipark/tfx/my_pipeline/models/features_test.py\n",
      "preprocessing.py -> /Users/sujipark/tfx/my_pipeline/models/preprocessing.py\n",
      "model_analysis.ipynb -> /Users/sujipark/tfx/my_pipeline/model_analysis.ipynb\n",
      "README.md -> /Users/sujipark/tfx/my_pipeline/README.md\n",
      ".gitignore -> /Users/sujipark/tfx/my_pipeline/.gitignore\n",
      "kubeflow_dag_runner.py -> /Users/sujipark/tfx/my_pipeline/kubeflow_dag_runner.py\n",
      "data_validation.ipynb -> /Users/sujipark/tfx/my_pipeline/data_validation.ipynb\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx template copy \\\n",
    "  --pipeline_name={PIPELINE_NAME} \\\n",
    "  --destination_path={PROJECT_DIR} \\\n",
    "  --model=taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyhkhhxY42xK"
   },
   "source": [
    "Change the working directory context in this notebook to the project directory.\n",
    "\n",
    "```bash\n",
    "cd ${PROJECT_DIR}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9e_g5rc42xL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sujipark/tfx/my_pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}\n",
    "# %cd /Users/sujipark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rKBRbE342xN"
   },
   "source": [
    "## Step 3. Browse your copied source files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QdiHik_w42xN"
   },
   "source": [
    "The TFX template provides basic scaffold files to build a pipeline, including Python source code, sample data, and Jupyter Notebooks to analyse the output of the pipeline. The `taxi` template uses the same *Chicago Taxi* dataset and ML model as the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
    "\n",
    "In Google Colab, you can browse files by clicking a folder icon on the left. Files should be copied under the project directoy, whose name is `my_pipeline` in this case. You can click directory names to see the content of the directory, and double-click file names to open them.\n",
    "\n",
    "Here is brief introduction to each of the Python files.\n",
    "-   `pipeline` - This directory contains the definition of the pipeline\n",
    "    -   `configs.py` — defines common constants for pipeline runners\n",
    "    -   `pipeline.py` — defines TFX components and a pipeline\n",
    "-   `models` - This directory contains ML model definitions.\n",
    "    -   `features.py`, `features_test.py` — defines features for the model\n",
    "    -   `preprocessing.py`, `preprocessing_test.py` — defines preprocessing\n",
    "        jobs using `tf::Transform`\n",
    "    -   `estimator` - This directory contains an Estimator based model.\n",
    "        -   `constants.py` — defines constants of the model\n",
    "        -   `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
    "    -   `keras` - This directory contains a Keras based model.\n",
    "        -   `constants.py` — defines constants of the model\n",
    "        -   `model.py`, `model_test.py` — defines DNN model using Keras\n",
    "-   `beam_dag_runner.py`, `kubeflow_dag_runner.py` — define runners for each orchestration engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmWtR_Eq42xQ"
   },
   "source": [
    "You might notice that there are some files with `_test.py` in their name. These are unit tests of the pipeline and it is recommended to add more unit tests as you implement your own pipelines.\n",
    "You can run unit tests by supplying the module name of test files with `-m` flag. You can usually get a module name by deleting `.py` extension and replacing `/` with `.`.  For example:\n",
    "\n",
    "```bash\n",
    "python -m models.features_test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0DzGg-642xQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests under Python 3.7.7: /Users/sujipark/opt/anaconda3/envs/tfx/bin/python\n",
      "[ RUN      ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "[       OK ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "[ RUN      ] FeaturesTest.testTransformedNames\n",
      "[       OK ] FeaturesTest.testTransformedNames\n",
      "[ RUN      ] FeaturesTest.test_session\n",
      "[  SKIPPED ] FeaturesTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.000s\n",
      "\n",
      "OK (skipped=1)\n",
      "Running tests under Python 3.7.7: /Users/sujipark/opt/anaconda3/envs/tfx/bin/python\n",
      "[ RUN      ] ModelTest.testBuildKerasModel\n",
      "2020-07-07 02:15:36.962634: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-07-07 02:15:36.985720: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe40204b880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-07-07 02:15:36.985761: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "/Users/sujipark/opt/anaconda3/envs/tfx/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:718: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(wrapped_dict, collections.Mapping):\n",
      "I0707 02:15:37.075826 4441214400 layer_utils.py:192] Model: \"model\"\n",
      "I0707 02:15:37.076002 4441214400 layer_utils.py:193] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.076076 4441214400 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I0707 02:15:37.076128 4441214400 layer_utils.py:195] ==================================================================================================\n",
      "I0707 02:15:37.076272 4441214400 layer_utils.py:190] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \n",
      "I0707 02:15:37.076326 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.076416 4441214400 layer_utils.py:190] trip_miles_xf (InputLayer)      [(None,)]            0                                            \n",
      "I0707 02:15:37.076467 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.076550 4441214400 layer_utils.py:190] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \n",
      "I0707 02:15:37.076600 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.076748 4441214400 layer_utils.py:190] dense_features (DenseFeatures)  (None, 1)            0           pickup_latitude_xf[0][0]         \n",
      "I0707 02:15:37.076807 4441214400 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I0707 02:15:37.076858 4441214400 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I0707 02:15:37.076905 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.077942 4441214400 layer_utils.py:190] dense (Dense)                   (None, 1)            2           dense_features[0][0]             \n",
      "I0707 02:15:37.078021 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.078175 4441214400 layer_utils.py:190] dense_1 (Dense)                 (None, 1)            2           dense[0][0]                      \n",
      "I0707 02:15:37.078234 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.078395 4441214400 layer_utils.py:190] dense_features_1 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \n",
      "I0707 02:15:37.078458 4441214400 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I0707 02:15:37.078509 4441214400 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I0707 02:15:37.078554 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.078647 4441214400 layer_utils.py:190] concatenate (Concatenate)       (None, 35)           0           dense_1[0][0]                    \n",
      "I0707 02:15:37.078700 4441214400 layer_utils.py:190]                                                                  dense_features_1[0][0]           \n",
      "I0707 02:15:37.078745 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.078874 4441214400 layer_utils.py:190] dense_2 (Dense)                 (None, 1)            36          concatenate[0][0]                \n",
      "I0707 02:15:37.078974 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.079117 4441214400 layer_utils.py:190] tf_op_layer_Squeeze (TensorFlow [(None,)]            0           dense_2[0][0]                    \n",
      "I0707 02:15:37.079175 4441214400 layer_utils.py:257] ==================================================================================================\n",
      "I0707 02:15:37.079701 4441214400 layer_utils.py:268] Total params: 40\n",
      "I0707 02:15:37.079773 4441214400 layer_utils.py:269] Trainable params: 40\n",
      "I0707 02:15:37.079825 4441214400 layer_utils.py:270] Non-trainable params: 0\n",
      "I0707 02:15:37.079869 4441214400 layer_utils.py:271] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.159754 4441214400 layer_utils.py:192] Model: \"model_1\"\n",
      "I0707 02:15:37.159883 4441214400 layer_utils.py:193] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.159953 4441214400 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I0707 02:15:37.160005 4441214400 layer_utils.py:195] ==================================================================================================\n",
      "I0707 02:15:37.160140 4441214400 layer_utils.py:190] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \n",
      "I0707 02:15:37.160197 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.160288 4441214400 layer_utils.py:190] trip_miles_xf (InputLayer)      [(None,)]            0                                            \n",
      "I0707 02:15:37.160338 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.160424 4441214400 layer_utils.py:190] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \n",
      "I0707 02:15:37.160475 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.160622 4441214400 layer_utils.py:190] dense_features_2 (DenseFeatures (None, 1)            0           pickup_latitude_xf[0][0]         \n",
      "I0707 02:15:37.160681 4441214400 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I0707 02:15:37.160733 4441214400 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I0707 02:15:37.160781 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.160951 4441214400 layer_utils.py:190] dense_3 (Dense)                 (None, 1)            2           dense_features_2[0][0]           \n",
      "I0707 02:15:37.161008 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.161167 4441214400 layer_utils.py:190] dense_features_3 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \n",
      "I0707 02:15:37.161229 4441214400 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \n",
      "I0707 02:15:37.161281 4441214400 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \n",
      "I0707 02:15:37.161327 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.161422 4441214400 layer_utils.py:190] concatenate_1 (Concatenate)     (None, 35)           0           dense_3[0][0]                    \n",
      "I0707 02:15:37.161478 4441214400 layer_utils.py:190]                                                                  dense_features_3[0][0]           \n",
      "I0707 02:15:37.161525 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.161677 4441214400 layer_utils.py:190] dense_4 (Dense)                 (None, 1)            36          concatenate_1[0][0]              \n",
      "I0707 02:15:37.161741 4441214400 layer_utils.py:259] __________________________________________________________________________________________________\n",
      "I0707 02:15:37.161843 4441214400 layer_utils.py:190] tf_op_layer_Squeeze_1 (TensorFl [(None,)]            0           dense_4[0][0]                    \n",
      "I0707 02:15:37.161897 4441214400 layer_utils.py:257] ==================================================================================================\n",
      "I0707 02:15:37.162441 4441214400 layer_utils.py:268] Total params: 38\n",
      "I0707 02:15:37.162521 4441214400 layer_utils.py:269] Trainable params: 38\n",
      "I0707 02:15:37.162576 4441214400 layer_utils.py:270] Non-trainable params: 0\n",
      "I0707 02:15:37.162626 4441214400 layer_utils.py:271] __________________________________________________________________________________________________\n",
      "[       OK ] ModelTest.testBuildKerasModel\n",
      "[ RUN      ] ModelTest.test_session\n",
      "[  SKIPPED ] ModelTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.238s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m models.features_test\n",
    "!{sys.executable} -m models.keras.model_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fK_C6C6g42xS"
   },
   "source": [
    "## Step 4. Run your first TFX pipeline\n",
    "\n",
    "You can create a pipeline using `pipeline create` command.\n",
    "```bash\n",
    "tfx pipeline create --engine=beam --pipeline_path=beam_dag_runner.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5YikNik42xX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating pipeline\n",
      "/Users/sujipark/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "Pipeline compiled successfully.\n",
      "Pipeline \"my_pipeline\" created successfully.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create --engine=beam --pipeline_path=beam_dag_runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kvZIn8142xZ"
   },
   "source": [
    "Then, you can run the created pipeline using `run create` command.\n",
    "```sh\n",
    "tfx run create --engine=beam --pipeline_name=\"${PIPELINE_NAME}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnTC_Rql42xZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "/Users/sujipark/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "INFO:absl:Component CsvExampleGen depends on [].\n",
      "INFO:absl:Component CsvExampleGen is scheduled.\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running driver for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "2020-07-07 02:57:50.043417: W ml_metadata/metadata_store/rdbms_metadata_access_object.cc:453] No property is defined for the Type\n",
      "INFO:absl:Running executor for CsvExampleGen\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Using 1 process(es) for Beam pipeline execution.\n",
      "INFO:absl:Processing input csv data /Users/sujipark/tfx/my_pipeline/data/* to TFExample.\n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Running publisher for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx run create --engine=beam --pipeline_name={PIPELINE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GM1G1Efw42xb"
   },
   "source": [
    "If successful, you'll see `Component CsvExampleGen is finished.` When you copy the template, only one component, CsvExampleGen, is included in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pfQcePs42xc"
   },
   "source": [
    "## Step 5. Add components for data validation.\n",
    "\n",
    "In this step, you will add components for data validation including `StatisticsGen`, `SchemaGen`, and `ExampleValidator`. If you are interested in data validation, please see [Get started with Tensorflow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started).\n",
    "\n",
    "We will modify copied pipeline definition in `pipeline/pipeline.py`. If you are working on your local environment, use your favorite editor to edit the file. If you are working on Google Colab, \n",
    "\n",
    ">**Click folder icon on the left to open `Files` view**.\n",
    "\n",
    ">**Click `my_pipeline` to open the directory and click `pipeline` directory to open and double-click `pipeline.py` to open the file**.\n",
    "\n",
    ">Find and uncomment the 3 lines which add `StatisticsGen`, `SchemaGen`, and `ExampleValidator` to the pipeline. (Tip: find comments containing `TODO(step 5):`).\n",
    "\n",
    "> Your change will be saved automatically in a few seconds. Make sure that the `*` mark in front of the `pipeline.py` disappeared in the tab title. **There is no save button or shortcut for the file editor in Colab. Python files in file editor can be saved to the runtime environment even in `playground` mode.**\n",
    "\n",
    "You now need to update the existing pipeline with modified pipeline definition. Use the `tfx pipeline update` command to update your pipeline, followed by the `tfx run create` command to create a new execution run of your updated pipeline.\n",
    "\n",
    "```sh\n",
    "# Update the pipeline\n",
    "tfx pipeline update --engine=beam --pipeline_path=beam_dag_runner.py\n",
    "# You can run the pipeline the same way.\n",
    "tfx run create --engine beam --pipeline_name \"${PIPELINE_NAME}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMsT-5EX42xc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Updating pipeline\n",
      "/Users/sujipark/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "Pipeline compiled successfully.\n",
      "Pipeline \"my_pipeline\" updated successfully.\n",
      "\u001b[0mCLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "/Users/sujipark/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "INFO:absl:Component CsvExampleGen depends on [].\n",
      "INFO:absl:Component CsvExampleGen is scheduled.\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running driver for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:Could not find matching artifact class for type 'Examples' (proto: 'id: 6\\nname: \"Examples\"\\nproperties {\\n  key: \"span\"\\n  value: INT\\n}\\nproperties {\\n  key: \"split_names\"\\n  value: STRING\\n}\\n'); generating an ephemeral artifact class on-the-fly. If this is not intended, please make sure that the artifact class for this type can be imported within your container or environment where a component is executed to consume this type.\n",
      "INFO:absl:Running publisher for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "!tfx pipeline update --engine=beam --pipeline_path=beam_dag_runner.py\n",
    "# You can run the pipeline the same way.\n",
    "!tfx run create --engine beam --pipeline_name {PIPELINE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUU5_3yR42xe"
   },
   "source": [
    "You should be able to see the output log from the added components. Our pipeline creates output artifacts in `tfx_pipeline_output/my_pipeline` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7p7CLDvD42xe"
   },
   "source": [
    "## Step 6. Add components for training.\n",
    "\n",
    "In this step, you will add components for training and model validation including `Transform`, `Trainer`, `ResolverNode`, `Evaluator`, and `Pusher`.\n",
    "\n",
    "> **Open `pipeline/pipeline.py`**. Find and uncomment 5 lines which add `Transform`, `Trainer`, `ResolverNode`, `Evaluator` and `Pusher` to the pipeline. (Tip: find `TODO(step 6):`)\n",
    "\n",
    "As you did before, you now need to update the existing pipeline with the modified pipeline definition. The instructions are the same as Step 5. Update the pipeline using `tfx pipeline update`, and create an execution run using `tfx run create`.\n",
    "\n",
    "\n",
    "```sh\n",
    "tfx pipeline update --engine=beam --pipeline_path=beam_dag_runner.py\n",
    "tfx run create --engine beam --pipeline_name \"${PIPELINE_NAME}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ik8JbnRq42xf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Updating pipeline\n",
      "/Users/sujipark/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "Pipeline compiled successfully.\n",
      "Pipeline \"my_pipeline\" updated successfully.\n",
      "\u001b[0mCLI\n",
      "Creating a run for pipeline: my_pipeline\n",
      "/Users/sujipark/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "INFO:absl:Component CsvExampleGen depends on [].\n",
      "INFO:absl:Component CsvExampleGen is scheduled.\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running driver for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "WARNING:absl:Could not find matching artifact class for type 'Examples' (proto: 'id: 6\\nname: \"Examples\"\\nproperties {\\n  key: \"span\"\\n  value: INT\\n}\\nproperties {\\n  key: \"split_names\"\\n  value: STRING\\n}\\n'); generating an ephemeral artifact class on-the-fly. If this is not intended, please make sure that the artifact class for this type can be imported within your container or environment where a component is executed to consume this type.\n",
      "INFO:absl:Running publisher for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update --engine=beam --pipeline_path=beam_dag_runner.py\n",
    "!tfx run create --engine beam --pipeline_name {PIPELINE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3L3NEPanUGY"
   },
   "source": [
    "When this execution run finishes successfully, you have now created and run your first TFX pipeline using Beam orchestrator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjcMwjov42xh"
   },
   "source": [
    "## Step 7. (*Optional*) Try BigQueryExampleGen.\n",
    "[BigQuery] is a serverless, highly scalable, and cost-effective cloud data warehouse. BigQuery can be used as a source for training examples in TFX. In this step, we will add `BigQueryExampleGen` to the pipeline.\n",
    "\n",
    "You need a [Google Cloud Platform](https://cloud.google.com/gcp/getting-started) account to use BigQuery. Please prepare a GCP project.\n",
    "\n",
    "Login to your project using colab auth library or `gcloud` utility.\n",
    "```sh\n",
    "# You need `gcloud` tool to login in local shell environment.\n",
    "gcloud auth login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2K7nuHZ4uNXc"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print('Authenticated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Win212lr03zP"
   },
   "source": [
    "You should specify your GCP project name to access BigQuery resources using TFX. Set `GOOGLE_CLOUD_PROJECT` environment variable to your project name.\n",
    "\n",
    "```sh\n",
    "export GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_NAME_HERE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vvpw_lGByxSx"
   },
   "outputs": [],
   "source": [
    "# Set your project name below.\n",
    "# WARNING! ENTER your project name before running this cell.\n",
    "%env GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_NAME_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhClPWEuuOaP"
   },
   "source": [
    "> **Open `pipeline/pipeline.py`**. Comment out `CsvExampleGen` and uncomment the line which create an instance of `BigQueryExampleGen`. You also need to uncomment `query` argument of the `create_pipeline` function.\n",
    "\n",
    "We need to specify which GCP project to use for BigQuery again, and this is done by setting `--project` in `beam_pipeline_args` when creating a pipeline.\n",
    "\n",
    "> **Open `pipeline/configs.py`**. Uncomment the definition of `BIG_QUERY__WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS` and `BIG_QUERY_QUERY`. You should replace the project id and the region value in this file with the correct values for your GCP project.\n",
    "\n",
    "> **Open `beam_dag_runner.py`**. Uncomment two arguments, `query` and `beam_pipeline_args`, for create_pipeline() method.\n",
    "\n",
    "Now the pipeline is ready to use BigQuery as an example source. Update the pipeline and create a run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8rOdC3r42xi"
   },
   "outputs": [],
   "source": [
    "!tfx pipeline update --engine=beam --pipeline_path=beam_dag_runner.py\n",
    "!tfx run create --engine beam --pipeline_name {PIPELINE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxYxNHLN42xo"
   },
   "source": [
    "## What's next: Ingest YOUR data to the pipeline.\n",
    "\n",
    "We made a pipeline for a model using the Chicago Taxi dataset. Now it's time to put your data into the pipeline.\n",
    "\n",
    "Your data can be stored anywhere your pipeline can access, including GCS, or BigQuery. You will need to modify the pipeline definition to access your data.\n",
    "\n",
    "1. If your data is stored in files, modify the `DATA_PATH` in `kubeflow_dag_runner.py` or `beam_dag_runner.py` and set it to the location of your files. If your data is stored in BigQuery, modify `BIG_QUERY_QUERY` in `pipeline/configs.py` to correctly query for your data.\n",
    "1. Add features in `models/features.py`.\n",
    "1. Modify `models/preprocessing.py` to [transform input data for training](https://www.tensorflow.org/tfx/guide/transform).\n",
    "1. Modify `models/keras/model.py` and `models/keras/constants.py` to [describe your ML model](https://www.tensorflow.org/tfx/guide/trainer).\n",
    "  - You can use an estimator based model, too. Change `RUN_FN` constant to `models.estimator.model.run_fn` in `pipeline/configs.py`.\n",
    "\n",
    "Please see [Trainer component guide](https://www.tensorflow.org/tfx/guide/trainer) for more introduction."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "template_beam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
